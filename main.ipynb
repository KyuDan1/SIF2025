{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyudan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pyttsx3\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Whisper 모델 로드 (ASR)\n",
    "asr_model = whisper.load_model(\"base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 18:26:24.913362: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-12 18:26:25.048362: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733995585.081293   56234 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733995585.092984   56234 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-12 18:26:25.239080: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_56234/1889782194.py:19: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=hf_pipeline)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# 1. Llama 모델 로드\n",
    "llama_model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # 원하는 Hugging Face 모델 경로\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_model_name)\n",
    "\n",
    "# 2. HuggingFace pipeline 생성\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=500,  # 생성할 텍스트의 최대 토큰 수\n",
    "    device=0  # GPU를 사용하려면 0으로 설정, CPU를 사용하려면 -1로 설정\n",
    ")\n",
    "\n",
    "# 3. LangChain의 HuggingFacePipeline으로 Wrapping\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# 이제 llm을 LangChain과 함께 사용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56234/2834149021.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/tmp/ipykernel_56234/2834149021.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=\"db\", embedding_function=embeddings)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Vector Store 세팅 (Chroma)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma(persist_directory=\"db\", embedding_function=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "import time\n",
    "import pygame\n",
    "\n",
    "# 4. 한국어 TTS 엔진 설정\n",
    "# 7. TTS (텍스트 -> 음성 출력)\n",
    "def text_to_speech_kor(text):\n",
    "    tts = gTTS(text=text, lang='ko')\n",
    "    filename = 'voice.mp3'\n",
    "    tts.save(filename)\n",
    "\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    pygame.mixer.music.unload()\n",
    "    os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Retrieval + QA 체인 구성\n",
    "template = \"\"\"\n",
    "당신은 반도체 전문가이며, 사용자의 질문에 한국어로 답합니다.\n",
    "아래는 관련 컨텍스트(영어)입니다. 이 컨텍스트를 기반으로 질문에 답해주세요.\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문(한국어): {question}\n",
    "\n",
    "답변을 한국어로 해주세요.\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import whisper\n",
    "import os\n",
    "\n",
    "def record_audio(file_name=\"input_audio.wav\", duration=5, samplerate=16000):\n",
    "    \"\"\"음성을 녹음하고 파일로 저장\"\"\"\n",
    "    print(\"음성을 녹음 중입니다...\")\n",
    "    audio = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype='int16')\n",
    "    sd.wait()  # 녹음이 끝날 때까지 대기\n",
    "    write(file_name, samplerate, audio)\n",
    "    print(f\"{file_name} 파일로 저장되었습니다.\")\n",
    "    return os.path.abspath(file_name)\n",
    "\n",
    "def transcribe_audio(file_path):\n",
    "    \"\"\"Whisper를 사용하여 음성을 텍스트로 변환\"\"\"\n",
    "    print(\"음성을 변환 중입니다...\")\n",
    "    model = whisper.load_model(\"base\")  # Whisper 모델 로드\n",
    "    result = model.transcribe(file_path, language=\"ko\")\n",
    "    return result[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "음성을 녹음 중입니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recording WAVE 'input_audio.wav' : Signed 16 bit Little Endian, Rate 44100 Hz, Stereo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "음성을 변환 중입니다...\n",
      "사용자 질문:  앤모스와 피모스의 차이가 뭐야\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56234/2867520911.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = qa_chain.run(user_question)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 답변: \n",
      "아래는 앤모스와 피모스의 차이가에 대한 설명입니다.\n",
      "\n",
      "아마도 bạn는 앤모스와 피모스를 미래의 인류에 대해 대화할 때 사용하는 two-factor authentication (2FA) 방법 중 하나입니다. 2FA는 두 가지 다른 단어를 사용하여, 사용자 identity를 xác minh하기 위해 사용됩니다. \n",
      "\n",
      "1.  **otp (one-time password)** : 2FA의 두 번째 단어는 OTP입니다. OTP는 6-8 자리 수자리 수를 사용하여 6자리 수자리 수를 2-3 번의 OTP를 사용하는 2FA 방법입니다. 이 방법은 사용자 identity에 대한 2FA를 제공합니다.\n",
      "2.  **sms (SMS)** : 2FA의 두 번째 단어는 SMS입니다. SMS는 사용자 identity에 대한 2FA를 제공하는 2FA 방법입니다. 사용자 identity를 2FA로 xác minh하기 위해 SMS를 사용합니다. \n",
      "\n",
      "이 두 가지 방법은 사용자 identity를 2FA로 xác minh하기 위해 사용됩니다. 앤모스와 피모스는 2FA를 사용하여 사용자 identity를 2FA로 xác minh합니다. \n",
      "\n",
      "다음은 앤모스와 피모스의 차이입니다.\n",
      "\n",
      "*   **안전한 identity management**: 앤모스와 피모스는 두 가지 다른 identity management 시스템을 사용합니다. 앤모스는 identity management system을 사용하여 사용자 identity를 관리합니다. 피모스는 identity management system을 사용하여 사용자 identity를 관리합니다.\n",
      "*   **대면의 identity management**: 앤모스는 대면의 identity management 시스템을 사용하여 사용자 identity를 관리합니다. 피모스는 대면의 identity management 시스템을 사용하여 사용자 identity를 관리합니다.\n",
      "*   **2FA**: 앤모스와 피모스는 2FA를 사용하여 사용자 identity를 2FA로 xác minh합니다. 앤모스는 OTP를 사용하여 2FA를 제공합니다. 피모스는 SMS를 사용하여 2FA를 제공합니다.\n",
      "*   **사용자 identity**: 앤모스와 피모스는 두 가지 다른 사용자 identity를 제공합니다. 앤모스는 사용자 identity를 2FA로 xác minh합니다. 피모스는 사용자 identity를 2FA\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM 답변:\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#3) TTS로 응답 음성 출력\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtext_to_speech_kor\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m, in \u001b[0;36mtext_to_speech_kor\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     14\u001b[0m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mmusic\u001b[38;5;241m.\u001b[39mplay()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mmusic\u001b[38;5;241m.\u001b[39mget_busy():\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mmusic\u001b[38;5;241m.\u001b[39munload()\n\u001b[1;32m     20\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(filename)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. 전체 흐름\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) 사용자 음성 입력\n",
    "    print(\"음성을 녹음 중입니다...\")\n",
    "    os.system(\"arecord -d 5 -f cd -t wav input_audio.wav\")  # 5초간 녹음 (Linux 시스템 가정)\n",
    "    user_question = transcribe_audio('input_audio.wav')  # Whisper로 음성 인식\n",
    "    print(\"사용자 질문:\", user_question)\n",
    "\n",
    "    # 2) RAG + LLM 처리\n",
    "    answer = qa_chain.run(user_question)\n",
    "    answer = answer.split('답변을 한국어로 해주세요.')[-1]\n",
    "    print(\"LLM 답변:\", answer)\n",
    "\n",
    "    #3) TTS로 응답 음성 출력\n",
    "    text_to_speech_kor(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 답변: \n",
      "1. nmosfet의 threshold voltage는 0.7V입니다.\n",
      "2. nmos\n"
     ]
    }
   ],
   "source": [
    "answer = qa_chain.run(\"nmosfet의 threshold voltage를 알려줘\")\n",
    "answer = answer.split('답변을 한국어로 해주세요.')[-1]\n",
    "print(\"LLM 답변:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
